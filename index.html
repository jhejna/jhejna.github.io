<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-SD5ERX98PV"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-SD5ERX98PV');
  </script>

  <title>Joey Hejna</title>
  
  <meta name="author" content="Joey Hejna">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Joey Hejna</name>
              </p>
              <!-- <p align="center">(Donald Joseph Hejna III)</p> -->

              <p>I'm a fourth year PhD student in the computer science department at Stanford University 
                advised by <a href="https://dorsa.fyi/">Dorsa Sadigh</a>. 
                Currently, I am a research intern at <a href="https://www.physical-intelligence.org/">Physical Intelligence</a>. 
                Previously, I was a student researcher in the Google Deepmind Robotics group and a recepient of the NDSEG Fellowship. 
                I completed my undergrad at UC Berkeley where I worked with Professors <a href="https://people.eecs.berkeley.edu/~pabbeel/">Pieter Abbeel</a> 
                and <a href="https://cs.nyu.edu/~lp91/">Lerrel Pinto</a>.
              </p>

              <p style="color:red; text-align:center">I am currently on the job market seeking full-time roles.</p>

              <p style="text-align:center">
                jhejna @ cs.stanford.edu &nbsp/&nbsp
                <a href="files/HejnaJoeyCV.pdf">CV</a> &nbsp/&nbsp
                <a href="https://github.com/jhejna">Github</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=y_sLoXoAAAAJ">Scholar</a> 
                <!-- &nbsp/&nbsp <a href="https://linkedin.com/in/jhejna">LinkedIn</a> -->
              </p>
            </td>
            <td style="padding:2.5%;width:36%;max-width:36%">
              <a href="images/profile.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/profile.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I'm broadly interested in learning for decision making and robotics. Papers (and preprints) are ordered by recency.
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <a href="https://rahulschand.github.io/iwr/"><img src='images/iwr.png' width="140">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://rahulschand.github.io/iwr/">
                <papertitle>Data Retrieval with Importance Weights for Few-Shot Imitation Learning</papertitle>
              </a>
              <br>
              <a href="https://amberxie88.github.io/">Amber Xie</a>
              <a href="https://rahulschand.github.io/">Rahul Schand</a>,
              <a href="https://dorsa.fyi/">Dorsa Sadigh</a>
              <strong>Joey Hejna</strong>,
              <br>
              <em>CoRL 2025</em>
              <br>
              <a href="https://arxiv.org/abs/2509.01657">paper</a> / <a href="https://rahulschand.github.io/iwr/">Website</a> / <a href="https://github.com/jhejna/importance-retrieval">code</a>
              <p></p>
              <p>
                Importance Weighted Retrieval (IWR) is a retrieval method that estimates importance weights, or the ratio between the target and prior data distributions, using Gaussian KDEs on top of existing retrieval methods.
              </p>
              </p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <a href="https://arxiv.org/abs/2506.19212"><img src='images/vlm_scaffolds.png' width="140">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2506.19212">
                <papertitle>Scaffolding Dexterous Manipulation with Vision Language Models</papertitle>
              </a>
              <br>
              <a href="https://www.linkedin.com/in/vincent-de-bakker/?locale=en_US">Vincent DeBakker</a>
              <strong>Joey Hejna</strong>,
              <a href="https://tylerlum.github.io/">Tyler Ga Wei Lum</a>,
              <a href="https://onur4229.github.io/">Onur Celik</a>,
              <a href="https://scholar.google.com/citations?user=2IovJsIAAAAJ&hl=de">Aleksandar Taranovic</a>,
              <a href="https://scholar.google.com/citations?user=_eEXIEcAAAAJ&hl=de">Denis Blessing</a>,
              <a href="https://scholar.google.com/citations?user=GL360kMAAAAJ&hl=en">Gerhard Neumann</a>,
              <a href="https://web.stanford.edu/~bohg/">Jeannette Bohg</a>,
              <a href="https://dorsa.fyi/">Dorsa Sadigh</a>
              <br>
              <em>NeurIPS 2025</em>
              <br>
              <a href="https://arxiv.org/abs/2506.19212">paper</a> / <a href="https://sites.google.com/view/dexterous-vlm-scaffolding">Website</a> / <a href="https://github.com/vdebakker/vlm-scaffolding">code</a>
              <p></p>
              <p>
                We use VLM generated plans to guide low-level RL for dexterous manipulation tasks.
              </p>
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <a href="https://arxiv.org/abs/2502.08623"><img src='images/deminf.gif' width="175">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2502.08623">
                <papertitle>Robot Data Curation with Mutual Information Estimators</papertitle>
              </a>
              <br>
              <strong>Joey Hejna</strong>,
              <a href="https://suvirpmirchandani.com/">Suvir Mirchandani</a>,
              <a href="https://abalakrishna123.github.io/">Ashwin Balakrishna</a>,
              <a href="https://anxie.github.io/">Annie Xie</a>,
              <a href="https://ayzaan.com/">Ayzaan Wahid</a>,
              <a href="https://jonathantompson.github.io/">Jonathan Tompson</a>,
              <a href="https://scholar.google.com/citations?user=GuU6oA4AAAAJ&hl=en">Pannag Sanketi</a>,
              <a href="https://robodhruv.github.io/">Dhruv Shah</a>,
              <a href="https://cdevin.github.io/">Coline Devin</a>*,
              <a href="https://dorsa.fyi/">Dorsa Sadigh</a>*
              <br>
              <em>RSS 2025</em>
              <br>
              <a href="https://arxiv.org/abs/2502.08623">paper</a> / <a href="https://jhejna.github.io/demonstration-info">website</a> / <a href="https://jhejna.github.io/demonstration-information">code</a>
              <p></p>
              <p>We introduce a new method for robot data curation that uses mutual information to select the most informative demonstrations. We show that this approach outperforms existing methods in both simulation and real-world settings.</p>
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <a href="https://arxiv.org/abs/2502.03717"><img src='images/lgpl.png' width="175">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2502.03717">
                <papertitle>Efficiently Generating Expressive Quadruped Behaviors via Language-Guided Preference Learning</papertitle>
              </a>
              <br>
              <a href="https://jadenvc.github.io/">Jaden Clark</a>,
              <strong>Joey Hejna</strong>,
              <a href="https://dorsa.fyi/">Dorsa Sadigh</a>
              <br>
              <em>ICRA 2025</em>
              <br>
              <a href="https://arxiv.org/abs/2502.03717">paper</a> / <a href="https://lgpl-gaits.github.io/">website</a>
              <p></p>
              <p>This paper introduces a novel approach that leverages priors generated by pre-trained LLMs alongside the 
                precision of preference learning. Our method, termed Language-Guided Preference Learning (LGPL), 
                uses LLMs to generate initial behavior samples, which are then refined through preference-based feedback to 
                learn behaviors that closely align with human expectations.
              </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <a href="https://arxiv.org/abs/2411.04549"><img src='images/gvl.gif' width="175">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2411.04549">
                <papertitle>Vision Language Models are In-Context Value Learners</papertitle>
              </a>
              <br>
              <a href="https://jasonma2016.github.io/">Jason Ma</a>,
              <strong>Joey Hejna</strong>, ... Google Deepmind Robotics ... ,
              <a href="https://dorsa.fyi/">Dorsa Sadigh</a>,
              <a href="https://fxia22.github.io/">Fei Xia</a>
              <br>
              <em>ICLR 2025</em>
              <br>
              <a href="https://arxiv.org/abs/2411.04549">paper</a> / <a href="https://generative-value-learning.github.io/">website</a>
              <p></p>
              <p>GVL poses value estimation as a temporal ordering problem over shuffled video frames; 
                this seemingly more challenging task encourages VLMs to more fully exploit their underlying semantic 
                and temporal grounding capabilities to differentiate frames based on their perceived task progress, consequently producing significantly better value predictions.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <a href="https://arxiv.org/abs/2406.00888"><img src='images/ditto.png' width="175">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2406.00888">
                <papertitle>Show, Don't Tell: Aligning Language Models with Demonstrated Feedback</papertitle>
              </a>
              <br>
              <a href="https://oshaikh.com/">Omar Shaikh*</a>,
              <a href="https://michelle123lam.github.io/">Michelle Lam*</a>,
              <strong>Joey Hejna*</strong>, 
              <a href="https://cs.stanford.edu/~shaoyj/"> Yijia Shao</a>,
              <a href="https://hci.stanford.edu/msb/">Michael Bernstein</a>, <a href="https://cs.stanford.edu/~diyiy/">Diyi Yang</a>
              <br>
              <em>ICLR 2025</em>
              <br>
              <a href="https://arxiv.org/abs/2406.00888">paper</a> / <a href="https://github.com/SALT-NLP/demonstrated-feedback">code</a>
              <p></p>
              <p>We introduce DITTO, an algorithm for few-shot adaptation or alignment of large language models using only two or three human demonstrations. We show that DITTO outperforms prior methods by a significant margin on automated evals and a real-world users study.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <a href="https://arxiv.org/abs/2408.14037"><img src='images/remix.gif' width="175">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2408.14037">
                <papertitle>ReMix: Optimizing Dataset Mixtures for Large Scale Imitation Learning</papertitle>
              </a>
              <br>
              <strong>Joey Hejna</strong>, 
              <a href="https://openreview.net/profile?id=~Chethan_Anand_Bhateja1">Chethan Bhateja</a>,
              <a href=""> Yichen Jiang</a>,
              <a href="https://kpertsch.github.io">Karl Pertsch</a>, 
              <a href="https://dorsa.fyi/">Dorsa Sadigh</a>
              <br>
              <em>CoRL 2024</em> <em style="color:red">(Best Paper Nominee, top 6 of 671)</em>
              <br>
              <a href="https://arxiv.org/abs/2406.00888">paper</a> / <a href="https://github.com/jhejna/remix">code</a>
              <p></p>
              <p>We introduce ReMix, a recipe for automatically curating data mixtures for training large scale robotic imitation learning policies. We demonstrate improvements over human-curated and uniform dataset mixtures.</p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <a href="https://openreview.net/pdf?id=XrxLGzF0lJ"><img src='images/autonomous_data.png' width="175">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://openreview.net/pdf?id=XrxLGzF0lJ">
                <papertitle>So You Think You Can Scale Up Autonomous Robot Data Collection?</papertitle>
              </a>
              <br>
              <a href="https://suvirpmirchandani.com/">Suvir Mirchandani</a>,
              <a href="https://suneel.belkhale.com/">Suneel Belkhale</a>,
              <strong>Joey Hejna</strong>, 
              <a href="">Evelyn Choi</a>,
              <a href="">Md Sazzad Islam</a>,
              <a href="https://dorsa.fyi/">Dorsa Sadigh</a>
              <br>
              <em>CoRL 2024</em>
              <br>
              <a href="https://openreview.net/pdf?id=XrxLGzF0lJ">paper</a>
              <p></p>
              <p>We investigate how well success-filtered autonomous imitation can work in real-world robotics settings.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <a href="https://arxiv.org/abs/2408.14037"><img src='images/motif.png' width="175">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle style="font-size:medium">MotIF: Motion Instruction Fine-tuning</papertitle>
              <br>
              <a href="https://minyoung1005.github.io/">Minyoung Hwnag</a>,
              <strong>Joey Hejna</strong>,
              <a href="https://dorsa.fyi/">Dorsa Sadigh</a>,
              <a href="https://yonatanbisk.com/">Yonatan Bisk</a>
              <br>
              <em>IEEE RA-L</em>
              <br> 
              <a href="https://motif-1k.github.io/">project page</a> /
              
              <a href="https://arxiv.org/abs/2409.10683v1">paper</a>
              <p></p>
              <p>
                Evaluating robot motions involves more than just the start and end states; it's about how the task is performed. We propose motion instruction fine-tuning (MotIF) and MotIF-1K dataset to improve VLMs' ability to understand nuanced robotic motions.
              </p>
            </td>
          </tr>
          

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <a href="https://arxiv.org/abs/2406.02900"><img src='images/scaling_laws.png' width="175">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2406.02900">
                <papertitle>Scaling Laws for Reward Model Overoptimization in Direct Alignment Algorithms</papertitle>
              </a>
              <br>
              <a href="https://rmrafailov.github.io/">Rafael Rafailov*</a>,
              <a href="https://yaswanthchittepu.github.io/">Yashwanth Chittepu*</a>,
              <a href="https://openreview.net/profile?id=~Ryan_Park1">Ryan Park*</a>,
              <a href="https://hari-sikchi.github.io/">Harshit Sikchi*</a>,
              <strong>Joey Hejna</strong>, 
              <a href="https://www.bradknox.net/">W. Bradley Knox</a>, <a href="https://ai.stanford.edu/~cbfinn/">Chelsea Finn</a>, <a href="https://people.cs.umass.edu/~sniekum/">Scott Niekum</a>
              <br>
              <em>ArXiv Pre-print</em>
              <br>
              <a href="https://arxiv.org/abs/2406.02900">paper</a>
              <p></p>
              <p>Through extensive experimentation and multiple model scales we characterize the over-optimization problem for direct alignment algorithms in large langauge models.</p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <a href="https://arxiv.org/abs/2403.12945"><img src='images/droid.gif' width="175">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2403.12945">
                <papertitle>DROID: A Large-Scale In-the-Wild Robot Manipulation Dataset</papertitle>
              </a>
              <br>
              <a href="https://github.com/AlexanderKhazatsky/">Alexander Khazatsky*</a>,
              <a href="https://kpertsch.github.io">Karl Pertsch*</a>, 
              ..., <strong>Joey Hejna</strong>, ...,
              <a href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a>, <a href="https://ai.stanford.edu/~cbfinn/">Chelsea Finn</a>
              <br>
              <em>RSS 2024</em>
              <br>
              <a href="https://droid-dataset.github.io/">website</a> / <a href="https://arxiv.org/abs/2403.12945">paper</a> / <a href="https://github.com/droid-dataset/droid">code</a>
              <p></p>
              <p>A large diverse dataset collected across multiple institutions totalling over 75K robot trajectories.  </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <p></p>
                <br>
                <br>
                <br>
                <a href="https://arxiv.org/abs/2404.12358"><img src='images/rtoqstar.png' width="175">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2404.12358">
                <papertitle>From r to Q*: Your Language Model is Secretly a Q-function</papertitle>
              </a>
              <br>
              <a href="https://rmrafailov.github.io/">Rafael Rafailov*</a>,
              <strong>Joey Hejna</strong>, 
              <a href="https://openreview.net/profile?id=~Ryan_Park1">Ryan Park</a>, <a href="https://ai.stanford.edu/~cbfinn/">Chelsea Finn</a>
              <br>
              <em>ArXiv Pre-print</em>
              <br>
              <a href="https://arxiv.org/abs/2404.12358">paper</a>
              <p></p>
              <p>Understanding direct alignment algorithms for LLMs through the perspective of a per-token Markov Decision Process.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <a href="https://arxiv.org/abs/2405.12213"><img src='images/octo.gif' width="175">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2405.12213">
                <papertitle>Octo: An Open-Source Generalist Robot Policy</papertitle>
              </a>
              <br>
              <a href="https://dibyaghosh.com">Dibya Ghosh*</a>, <a href="https://homerwalke.com/">Homer Walke</a>, 
              <a href="https://kpertsch.github.io">Karl Pertsch*</a>, <a href="https://kevin.black">Kevin Black*</a>, 
              <a href="https://sudeepdasari.github.io">Sudeep Dasari</a>, <strong>Joey Hejna</strong>, <a href="https://charlesxu0124.github.io">Charles Xu</a>, <a href="https://people.eecs.berkeley.edu/~jianlanluo/">Jianlan Luo</a>, Tobias Kreiman, You Liang Tan, <a href="https://dorsa.fyi/">Dorsa Sadigh</a>,  <a href="https://ai.stanford.edu/~cbfinn/">Chelsea Finn</a>, <a href="https://people.eecs.berkeley.edu/~svlevine/">Sergey Levine</a>
              <br>
              <em>RSS 2024</em>
              <br>
              <a href="https://octo-models.github.io/">website</a> / <a href="https://arxiv.org/abs/2405.12213">paper</a> / <a href="https://github.com/octo-models/octo">code</a>
              <p></p>
              <p>Laying the groundwork for a foundation model for robotics.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <a href="https://arxiv.org/abs/2310.13639"><img src='images/cpl.png' width="175">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2310.13639">
                <papertitle>Contrastive Preference Learning: Learning from Human Feedback without RL</papertitle>
              </a>
              <br>
              <strong>Joey Hejna</strong>, <a href="https://rmrafailov.github.io/">Rafael Rafailov</a>*, <a href="https://hari-sikchi.github.io/">Harshit Sikchi</a>*, <a href="https://ai.stanford.edu/~cbfinn/">Chelsea Finn</a>, <a href="https://people.cs.umass.edu/~sniekum/">Scott Niekum</a>, <a href="https://www.bradknox.net/">W. Bradley Knox</a>, <a href="https://dorsa.fyi/">Dorsa Sadigh</a>
              <br>
              <em>ICLR 2024</em>
              <br>
              <a href="https://arxiv.org/abs/2310.13639">paper</a> / <a href="https://github.com/jhejna/cpl">code</a>
              <p></p>
              <p>Reduces Reinforcement Learning from Human Feedback to contrastive learning under the regret model of human preferences, which has recently been shown to be more accurate than the widely accepted reward model. Unlike many RLHF methods, CPL is fully off-policy and works on arbitrary MDPs.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <a href="https://arxiv.org/abs/2305.15363"><img src='images/hejna2023inverse.png' width="175">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2305.15363">
                <papertitle>Inverse Preference Learning: Preference-based RL without a Reward Function</papertitle>
              </a>
              <br>
              <strong>Joey Hejna</strong>, <a href="https://dorsa.fyi/">Dorsa Sadigh</a>
              <br>
              <em>NeurIPS 2023</em>
              <br>
              <a href="https://arxiv.org/abs/2305.15363">paper</a> / <a href="https://github.com/jhejna/inverse-preference-learning">code</a>
              <p></p>
              <p>Approaches to preference-based RL typically work in two phases: first a reward function is learned, then it is maximized using a vanilla RL algorithm. We introduce the Inverse Preference Learning framework, where we directly learn a Q-function that models the user's preferences without explicitly learning a reward function.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <a href="https://arxiv.org/abs/2304.13774"><img src='images/dwsl_icon.gif' width="175">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2304.13774">
                <papertitle>Distance Weighted Supervised Learning for Offline Interaction Data</papertitle>
              </a>
              <br>
              <strong>Joey Hejna</strong>, <a href="https://scholar.google.com/citations?user=SnTMyGUAAAAJ&hl=en">Jensen Gao</a>, <a href="https://dorsa.fyi/">Dorsa Sadigh</a>
              <br>
              <em>ICML 2023</em>
              <br>
              <a href="https://arxiv.org/abs/2304.13774">paper</a> / <a href="https://sites.google.com/view/dwsl">website</a> / <a href="https://github.com/jhejna/dwsl">code</a>
              <p></p>
              <p>We introduce DWSL, an algorithm for offline goal-conditioned reinforcement learning that uses only supervised objectives while still learning a constrained optimal policy. DWSL performs particularly well on high-dimensional image domains and seems robust to hyperparamters.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <a href="https://arxiv.org/abs/2301.02328L"><img src='images/xql_icon.png' width="175">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2301.02328">
                <papertitle>Extreme Q-Learning: MaxEnt RL without Entropy</papertitle>
              </a>
              <br>
              <a href="https://divyanshgarg.com/">Divyansh Garg*</a>, <strong>Joey Hejna*</strong>,
              <a href="https://scholar.google.com/citations?user=ectPLEUAAAAJ&hl=en">Matthieu Geist</a>,<a href="https://cs.stanford.edu/~ermon/">Stefano Ermon</a>
              <br>
              *Equal Contribution
              <br>
              <em>ICLR 2023</em> <em style="color:red">(Notable, Top 5% of Submissions)</em>
              <br>
              <a href="https://arxiv.org/abs/2301.02328">paper</a>/ <a href="https://div99.github.io/XQL/">website</a> / <a href="https://github.com/div99/xql">code</a>
              <p></p>
              <p>We introduce a novel framework for Q-learning that models the maximal soft-values without needing to sample from a policy and improves performance in online and offline RL settings.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <a href="ttps://openreview.net/pdf?id=IKC5TfXLuW0"><img src='images/few_shot_pref_icon.png' width="175">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="ttps://openreview.net/pdf?id=IKC5TfXLuW0">
                <papertitle>Few-Shot Preference Learning for Human-in-the-Loop RL</papertitle>
              </a>
              <br>
              <strong>Joey Hejna</strong>,
              <a href="https://dorsa.fyi/">Dorsa Sadigh</a>
              <br>
              <em>CoRL 2022</em>  
              <br>
              <a href="https://openreview.net/pdf?id=IKC5TfXLuW0">paper</a>  / <a href="https://sites.google.com/view/few-shot-preference-rl/home">website</a> / <a href="https://github.com/jhejna/few-shot-preference-rl">code</a>
              <p></p>
              <p>Pretraining preference models greatly reduces query-complexity, enabling humans to teach robots with a reasonable amount of feedback.</p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <a href="https://arxiv.org/abs/2306.12554"><img src='images/lang_pred.png' width="120">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2306.12554">
                <papertitle>Improving Long-Horizon Imitation through Instruction Prediction</papertitle>
              </a>
              <br>
              <strong>Joey Hejna</strong>,
              <a href="https://people.eecs.berkeley.edu/~pabbeel/">Pieter Abbeel</a>,
              <a href="https://cs.nyu.edu/~lp91/">Lerrel Pinto</a>
              <br>
              <em>AAAI 2023</em>  
              <br>
              <a href="https://arxiv.org/abs/2306.12554">paper</a> / <a href="https://github.com/jhejna/instruction-prediction">code</a>
              <p></p>
              <p>We show that predicting instructions along with actions drastically improves performance in combinatorially complex long-horizon imitation settings.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <a href="https://openreview.net/pdf?id=CGQ6ENUMX6"><img src='images/tame.gif' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://openreview.net/pdf?id=CGQ6ENUMX6">
                <papertitle>Task-Agnostic Morphology Evolution</papertitle>
              </a>
              <br>
              <strong>Donald J. Hejna III</strong>,
              <a href="https://people.eecs.berkeley.edu/~pabbeel/">Pieter Abbeel</a>,
              <a href="https://cs.nyu.edu/~lp91/">Lerrel Pinto</a>
              <br>
              <em>Accepted to ICLR 2021</em>  
              <br>
              <a href="https://openreview.net/pdf?id=CGQ6ENUMX6">paper</a> / <a href="https://sites.google.com/view/task-agnostic-evolution">website</a> / <a href="https://github.com/jhejna/morphology-opt">code</a>
              <p></p>
              <p>Better robot strucutres hold the promise of better performance. We propose a new algorithm, TAME, that is able to evolve morphologies without any task specification. This is accomplished using an information theoretic objective that efficiently ranks morphologies based on their ability to explore and control their environment.</p>
            </td>
          </tr>
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <a href="https://arxiv.org/abs/2003.01709"><img src="images/hierarchical_transfer.png" width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2003.01709">
                <papertitle>Hierarchically Decoupled Imitation for Morphological Transfer</papertitle>
              </a>
              <br>
              <strong>Donald J. Hejna III</strong>,
              <a href="https://people.eecs.berkeley.edu/~pabbeel/">Pieter Abbeel</a>,
              <a href="https://cs.nyu.edu/~lp91/">Lerrel Pinto</a>
              <br>
              <em>Accepted to ICML 2020</em>  
              <br>
              <a href="https://arxiv.org/abs/2003.01709">paper</a> / <a href="https://sites.google.com/berkeley.edu/morphology-transfer">website</a> / <a href="https://github.com/jhejna/hierarchical_morphology_transfer">code</a> / <a href="https://icml.cc/virtual/2020/poster/6826">talk</a>
              <p></p>
              <p>We propose transferring RL policies across agents using a hierarchical framework. Then, to remedy poor zero-shot transfer performance we introduce two additional imitation objectives.</p>
            </td>
          </tr>
        
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Projects</heading>
          </td>
        </tr>
      </tbody></table>
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        
        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <a href="https://github.com/jhejna/research-lightning"><img src='images/bolt_small.png' width="160">
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://github.com/jhejna/research-lightning">
              <papertitle>Research Lightning</papertitle>
            </a>
            <br>
            <strong>Donald J. Hejna III</strong>
            <br>
            <em>Open source project</em>
            <br>
            <a href="https://github.com/jhejna/research-lightning">code</a>
            <p></p>
            <p>A lightweight framework for general deep-learning research in pytorch.</p>
          </td>
        </tr>

        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <a href="https://github.com/jhejna/openx"><img src='images/openx.png' width="160">
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://github.com/jhejna/openx">
              <papertitle>OpenX Repository</papertitle>
            </a>
            <br>
            <strong>Donald J. Hejna III</strong>
            <br>
            <em>Open source project</em>
            <br>
            <a href="https://github.com/jhejna/openx">code</a>
            <p></p>
            <p>A lightweight framework for general deep-learning research in pytorch.</p>
          </td>
        </tr>

        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <a href="files/disentanglement.pdf"><img src='images/disentanglement.png' width="155">
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="files/disentanglement.pdf">
              <papertitle>Improving Latent Representations via Explicit Disentanglement</papertitle>
            </a>
            <br>
            <strong>Donald J. Hejna III*</strong>,
            Ashwin Vangipuram*,
            Kara Liu*
            <br>
            <em>Course Project, CS 294-158 Unsupervised Learning</em>, Spring 2020  
            <br>
            <a href="files/disentanglement.pdf">paper</a>
            <p></p>
            <p>We examine and compare three methods for explicitly disentangling learned latent representations in VAE models.</p>
          </td>
        </tr>

      </tbody></table>

      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Awards</heading>
            <ul>
              <li>National Defense Science and Engineering Graduate Scholarship (NDSEG) 2021, roughly 5% selection rate.</li>
              <li><a href="https://cra.org/about/awards/outstanding-undergraduate-researcher-award/#2021">Honorable mention</a> for the 2021 CRA Outstanding Undergraduate Researcher Award</li>
              <li>Highest Degree Honors in Engineering at UC Berkeley Spring 2021, top 3% of the graduating class.</li>
              <li>UC Berkeley Regents and Chancellors Scholarship</li>
              <li>Rambus Innovator of the Future 2017</li>
            </ul>
          </td>
        </tr>
      </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Industry</heading>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/pi.png' width="150">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Research Intern, Physical Intelligence</papertitle>
              <br>
              Summer 2025
              <br>
              <p>Working on data collection, labeling, and training more reactive policies.</p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='images/gdm.png' width="150">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Student Researcher, Google DeepMind Robotics</papertitle>
              <br>
              Summer 2024
              <br>
              <p>Working on the machine learning and robotics team.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='img/citadel/citadel.jpg' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Intern, Citadel Global Quantitative Strategies</papertitle>
              <br>
              Summer 2019
              <br>
              <p>Developed C++ systems for trading APIs and monitoring systems. Worked on optimizing memory usage of large model training.</p>
            </td>
          </tr>
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='img/intelintern/cardboard.png' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Intern, Intel Artificial Intelligence Group</papertitle>
              <br>
              Summer 2018  
              <br>
              <a href="https://www.intel.com/content/www/us/en/artificial-intelligence/posts/unlocking-aws-deeplens-with-the-openvino-toolkit.html#gs.a70jlt">blog post</a>
              <p></p>
              <p>Worked on demo systems for Intel's OpenVino model optimization system in the AWS DeepLens. Explored systems for gradient based explanations of deep networks.</p>
            </td>
          </tr>

        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Teaching and Service</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/reviewer.png" alt="reviewer" width="160" height="160">
            </td>
            <td width="75%" valign="center">
              <papertitle>Reviewer</papertitle>
              <br>
              <br>
              NeurIPS 2023 Outstanding Reviewer, ICML 2024 Outstanding Reviewer, CoRL 2024, IEEE RA-L,
              RL-Brew at RLC 2024, ICRA 2025, ICML 2023, ICLR 2025
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/berkeley.png" alt="berkeley" width="160" height="160">
            </td>
            <td width="75%" valign="center">
              <papertitle>Computer Science Department at Stanford and UC Berkeley</papertitle>
              <br>
              <br>
              <a href="https://stanford-cs221.github.io/autumn2024/" >Course Assistant, CS 221: Artificial Intelligence, Autumn 2024</a>
              <br>
              <br>
              <a href="https://stanford-cs221.github.io/autumn2023/" >Head Course Assistant, CS 221: Artificial Intelligence, Autumn 2023</a>
              <br>
              <br>
              <a href="https://inst.eecs.berkeley.edu/~ee127/fa20/" >Teaching Assistant, EECS 127: Optimization Models, Fall 2020</a>
              <br>
              <br>
              <a href="https://people.eecs.berkeley.edu/~jrs/189/">Teaching Assistant, EECS 189: Machine Learning, Spring 2020</a>
              <br>
              <br>
              <a href="https://www.fa19.eecs70.org/">Teaching Assistant, CS 70: Discrete Math and Probability Theory, Fall 2019</a>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/teaching.png" alt="teaching resources" width="160" height="160">
            </td>
            <td width="75%" valign="center">
              <papertitle>Public Resources</papertitle>
              <br>
              <br>
              <a href="https://jhejna.github.io/mlbook">Introductory ML Notes</a>
              <br>
              <br>
              <a href="https://github.com/jhejna/mlworkshop">Deep Learning Workshop</a>
              <br>
              <br>
              <a href="https://github.com/jhejna/rlworkshop">Reinforcement Learning Workshop</a>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Website source taken from <a href="https://github.com/jonbarron/jonbarron_website">here</a>.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
