<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-SD5ERX98PV"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'G-SD5ERX98PV');
  </script>

  <title>Joey Hejna</title>
  
  <meta name="author" content="Joey Hejna">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Joey Hejna</name>
              </p>
              <p align="center">(Donald Joseph Hejna III)</p>
              
              <p>I'm a third year PhD student in the computer science department at Stanford University advised by <a href="https://dorsa.fyi/">Dorsa Sadigh</a>. My research is supported by an NDSEG Fellowship. I completed my undergrad at UC Berkeley where I worked with Professors <a href="https://people.eecs.berkeley.edu/~pabbeel/">Pieter Abbeel</a> and <a href="https://cs.nyu.edu/~lp91/">Lerrel Pinto</a>.
              </p>
              <p style="text-align:center">
                jhejna @ cs.stanford.edu &nbsp/&nbsp
                <a href="files/HejnaJoeyResume.pdf">Resume</a> &nbsp/&nbsp
                <a href="https://github.com/jhejna">Github</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=y_sLoXoAAAAJ">Scholar</a> &nbsp/&nbsp
                <a href="https://linkedin.com/in/jhejna">LinkedIn</a>
              </p>
            </td>
            <td style="padding:2.5%;width:36%;max-width:36%">
              <a href="images/profile.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/profile.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I'm broadly interested in learning for decision making and robotics. Papers (and preprints) are ordered by recency.
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <a href="https://arxiv.org/abs/2310.13639"><img src='images/cpl.png' width="175">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="hhttps://arxiv.org/abs/2310.13639">
                <papertitle>Contrastive Preference Learning: Learning from Human Feedback without RL</papertitle>
              </a>
              <br>
              <strong>Joey Hejna</strong>, <a href="https://rmrafailov.github.io/">Rafael Rafailov</a>*, <a href="https://hari-sikchi.github.io/">Harshit Sikchi</a>*, <a href="https://ai.stanford.edu/~cbfinn/">Chelsea Finn</a>, <a href="https://people.cs.umass.edu/~sniekum/">Scott Niekum</a>, <a href="https://www.bradknox.net/">W. Bradley Knox</a> <a href="https://dorsa.fyi/">Dorsa Sadigh</a>
              <br>
              <em>ArXiv Preprint</em>
              <br>
              <a href="https://arxiv.org/abs/2310.13639">paper</a> / <a href="https://github.com/jhejna/cpl">code</a>
              <p></p>
              <p>Reduces Reinforcement Learning from Human Feedback to contrastive learning under the regret model of human preferences, which has recently been shown to be more accurate than the widely accepted reward model. Unlike many RLHF methods, CPL is fully off-policy and works on arbitrary MDPs.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <a href="https://arxiv.org/abs/2305.15363"><img src='images/hejna2023inverse.png' width="175">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2305.15363">
                <papertitle>Inverse Preference Learning: Preference-based RL without a Reward Function</papertitle>
              </a>
              <br>
              <strong>Joey Hejna</strong>, <a href="https://dorsa.fyi/">Dorsa Sadigh</a>
              <br>
              <em>NeurIPS 2023</em>
              <br>
              <a href="https://arxiv.org/abs/2305.15363">paper</a> / <a href="https://github.com/jhejna/inverse-preference-learning">code</a>
              <p></p>
              <p>Approaches to preference-based RL typically work in two phases: first a reward function is learned, then it is maximized using a vanilla RL algorithm. We introduce the Inverse Preference Learning framework, where we directly learn a Q-function that models the user's preferences without explicitly learning a reward function.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <a href="https://arxiv.org/abs/2304.13774"><img src='images/dwsl_icon.gif' width="175">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2304.13774">
                <papertitle>Distance Weighted Supervised Learning for Offline Interaction Data</papertitle>
              </a>
              <br>
              <strong>Joey Hejna</strong>, <a href="https://scholar.google.com/citations?user=SnTMyGUAAAAJ&hl=en">Jensen Gao</a>, <a href="https://dorsa.fyi/">Dorsa Sadigh</a>
              <br>
              <em>ICML 2023</em>
              <br>
              <a href="https://arxiv.org/abs/2304.13774">paper</a> / <a href="https://sites.google.com/view/dwsl">website</a> / <a href="https://github.com/jhejna/dwsl">code</a>
              <p></p>
              <p>We introduce DWSL, an algorithm for offline goal-conditioned reinforcement learning that uses only supervised objectives while still learning a constrained optimal policy. DWSL performs particularly well on high-dimensional image domains and seems robust to hyperparamters.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <a href="https://arxiv.org/abs/2301.02328L"><img src='images/xql_icon.png' width="175">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2301.02328">
                <papertitle>Extreme Q-Learning: MaxEnt RL without Entropy</papertitle>
              </a>
              <br>
              <a href="https://divyanshgarg.com/">Divyansh Garg*</a>, <strong>Joey Hejna*</strong>,
              <a href="https://scholar.google.com/citations?user=ectPLEUAAAAJ&hl=en">Matthieu Geist</a>,<a href="https://cs.stanford.edu/~ermon/">Stefano Ermon</a>
              <br>
              *Equal Contribution
              <br>
              <em>ICLR 2023</em> <em style="color:red">(Notable, Top 5% of Submissions)</em>
              <br>
              <a href="https://arxiv.org/abs/2301.02328">paper</a>/ <a href="https://div99.github.io/XQL/">website</a> / <a href="https://github.com/div99/xql">code</a>
              <p></p>
              <p>We introduce a novel framework for Q-learning that models the maximal soft-values without needing to sample from a policy and improves performance in online and offline RL settings.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <a href="ttps://openreview.net/pdf?id=IKC5TfXLuW0"><img src='images/few_shot_pref_icon.png' width="175">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="ttps://openreview.net/pdf?id=IKC5TfXLuW0">
                <papertitle>Few-Shot Preference Learning for Human-in-the-Loop RL</papertitle>
              </a>
              <br>
              <strong>Donald J. Hejna III</strong>,
              <a href="https://dorsa.fyi/">Dorsa Sadigh</a>
              <br>
              <em>CoRL 2022</em>  
              <br>
              <a href="https://openreview.net/pdf?id=IKC5TfXLuW0">paper</a>  / <a href="https://sites.google.com/view/few-shot-preference-rl/home">website</a> / <a href="https://github.com/jhejna/few-shot-preference-rl">code</a>
              <p></p>
              <p>Pretraining preference models greatly reduces query-complexity, enabling humans to teach robots with a reasonable amount of feedback.</p>
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <a href="https://arxiv.org/abs/2306.12554"><img src='images/lang_pred.png' width="120">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2306.12554">
                <papertitle>Improving Long-Horizon Imitation through Instruction Prediction</papertitle>
              </a>
              <br>
              <strong>Joey Hejna</strong>,
              <a href="https://people.eecs.berkeley.edu/~pabbeel/">Pieter Abbeel</a>,
              <a href="https://cs.nyu.edu/~lp91/">Lerrel Pinto</a>
              <br>
              <em>AAAI 2023</em>  
              <br>
              <a href="https://arxiv.org/abs/2306.12554">paper</a> / <a href="https://github.com/jhejna/instruction-prediction">code</a>
              <p></p>
              <p>We show that predicting instructions along with actions drastically improves performance in combinatorially complex long-horizon imitation settings.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <a href="https://openreview.net/pdf?id=CGQ6ENUMX6"><img src='images/tame.gif' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://openreview.net/pdf?id=CGQ6ENUMX6">
                <papertitle>Task-Agnostic Morphology Evolution</papertitle>
              </a>
              <br>
              <strong>Donald J. Hejna III</strong>,
              <a href="https://people.eecs.berkeley.edu/~pabbeel/">Pieter Abbeel</a>,
              <a href="https://cs.nyu.edu/~lp91/">Lerrel Pinto</a>
              <br>
              <em>Accepted to ICLR 2021</em>  
              <br>
              <a href="https://openreview.net/pdf?id=CGQ6ENUMX6">paper</a> / <a href="https://sites.google.com/view/task-agnostic-evolution">website</a> / <a href="https://github.com/jhejna/morphology-opt">code</a>
              <p></p>
              <p>Better robot strucutres hold the promise of better performance. We propose a new algorithm, TAME, that is able to evolve morphologies without any task specification. This is accomplished using an information theoretic objective that efficiently ranks morphologies based on their ability to explore and control their environment.</p>
            </td>
          </tr>
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <a href="https://arxiv.org/abs/2003.01709"><img src="images/hierarchical_transfer.png" width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2003.01709">
                <papertitle>Hierarchically Decoupled Imitation for Morphological Transfer</papertitle>
              </a>
              <br>
              <strong>Donald J. Hejna III</strong>,
              <a href="https://people.eecs.berkeley.edu/~pabbeel/">Pieter Abbeel</a>,
              <a href="https://cs.nyu.edu/~lp91/">Lerrel Pinto</a>
              <br>
              <em>Accepted to ICML 2020</em>  
              <br>
              <a href="https://arxiv.org/abs/2003.01709">paper</a> / <a href="https://sites.google.com/berkeley.edu/morphology-transfer">website</a> / <a href="https://github.com/jhejna/hierarchical_morphology_transfer">code</a> / <a href="https://icml.cc/virtual/2020/poster/6826">talk</a>
              <p></p>
              <p>We propose transferring RL policies across agents using a hierarchical framework. Then, to remedy poor zero-shot transfer performance we introduce two additional imitation objectives.</p>
            </td>
          </tr>
        
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Projects</heading>
          </td>
        </tr>
      </tbody></table>
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
        
        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <a href="https://github.com/jhejna/research-lightning"><img src='images/bolt_small.png' width="160">
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="https://github.com/jhejna/research-lightning">
              <papertitle>Research Lightning</papertitle>
            </a>
            <br>
            <strong>Donald J. Hejna III</strong>
            <br>
            <em>Open source project</em>
            <br>
            <a href="https://github.com/jhejna/research-lightning">code</a>
            <p></p>
            <p>A lightweight framework for general deep-learning research in pytorch.</p>
          </td>
        </tr>

        <tr>
          <td style="padding:20px;width:25%;vertical-align:middle">
            <div class="one">
              <a href="files/disentanglement.pdf"><img src='images/disentanglement.png' width="155">
            </div>
          </td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a href="files/disentanglement.pdf">
              <papertitle>Improving Latent Representations via Explicit Disentanglement</papertitle>
            </a>
            <br>
            <strong>Donald J. Hejna III*</strong>,
            Ashwin Vangipuram*,
            Kara Liu*
            <br>
            <em>Course Project, CS 294-158 Unsupervised Learning</em>, Spring 2020  
            <br>
            <a href="files/disentanglement.pdf">paper</a>
            <p></p>
            <p>We examine and compare three methods for explicitly disentangling learned latent representations in VAE models.</p>
          </td>
        </tr>

      </tbody></table>

      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Awards</heading>
            <ul>
              <li>National Defense Science and Engineering Graduate Scholarship (NDSEG) 2021, roughly 2% selection rate.</li>
              <li><a href="https://cra.org/about/awards/outstanding-undergraduate-researcher-award/#2021">Honorable mention</a> for the 2021 CRA Outstanding Undergraduate Researcher Award</li>
              <li>Highest Degree Honors in Engineering at UC Berkeley Spring 2021, top 3% of the graduating class.</li>
              <li>UC Berkeley Regents and Chancellors Scholarship</li>
              <li>Rambus Innovator of the Future 2017</li>
            </ul>
          </td>
        </tr>
      </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Industry</heading>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='img/citadel/citadel.jpg' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Intern, Citadel Global Quantitative Strategies</papertitle>
              <br>
              Summer 2019
              <br>
              <p>Developed C++ systems for trading APIs and monitoring systems. Worked on optimizing memory usage of large model training.</p>
            </td>
          </tr>
          
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img src='img/intelintern/cardboard.png' width="160">
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Intern, Intel Artificial Intelligence Group</papertitle>
              <br>
              Summer 2018  
              <br>
              <a href="https://www.intel.com/content/www/us/en/artificial-intelligence/posts/unlocking-aws-deeplens-with-the-openvino-toolkit.html#gs.a70jlt">blog post</a>
              <p></p>
              <p>Worked on demo systems for Intel's OpenVino model optimization system in the AWS DeepLens. Explored systems for gradient based explanations of deep networks.</p>
            </td>
          </tr>

        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Teaching</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/berkeley.png" alt="berkeley" width="160" height="160">
            </td>
            <td width="75%" valign="center">
              <papertitle>UC Berkeley EECS Department</papertitle>
              <br>
              <br>
              <a href="https://inst.eecs.berkeley.edu/~ee127/fa20/" >Teaching Assistant, EECS 127: Optimization Models, Fall 2020</a>
              <br>
              <br>
              <a href="https://people.eecs.berkeley.edu/~jrs/189/">Teaching Assistant, EECS 189: Machine Learning, Spring 2020</a>
              <br>
              <br>
              <a href="https://www.fa19.eecs70.org/">Teaching Assistant, CS 70: Discrete Math and Probability Theory, Fall 2019</a>
            </td>
          </tr>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/teaching.png" alt="teaching resources" width="160" height="160">
            </td>
            <td width="75%" valign="center">
              <papertitle>Public Resources</papertitle>
              <br>
              <br>
              <a href="https://jhejna.github.io/mlbook">Introductory ML Notes</a>
              <br>
              <br>
              <a href="https://github.com/jhejna/mlworkshop">Deep Learning Workshop</a>
              <br>
              <br>
              <a href="https://github.com/jhejna/rlworkshop">Reinforcement Learning Workshop</a>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                Website source taken from <a href="https://github.com/jonbarron/jonbarron_website">here</a>.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
